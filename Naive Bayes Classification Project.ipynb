{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import operator#Has predefined functions for sorting dictionaries\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory='20_newsgroups'#Storing the document name as a string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore=[]  \n",
    "ignore+=['subject:','from:', 'date:', 'newsgroups:', 'message-id:', 'lines:', 'path:', 'organization:', \n",
    "            'would', 'writes:', 'references:', 'article', 'sender:', 'nntp-posting-host:', 'people', \n",
    "            'university', 'think', 'xref:', 'cantaloupe.srv.cs.cmu.edu', 'could', 'distribution:', 'first', \n",
    "            'anyone','world', 'really', 'since', 'right', 'believe', 'still', \n",
    "            \"max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\"]#These words have no role in classification\n",
    "ignore+=list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_folders=sorted(os.listdir(os.path.join(directory)))\n",
    "len(all_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={}#Creating a dictionary to store name of folder as key and the documents as values in form of a list \n",
    "for folder in all_folders:\n",
    "    data[folder]=[]\n",
    "    for file in os.listdir(os.path.join(directory,folder)):\n",
    "        with open(os.path.join(directory,folder,file),encoding='latin-1') as opened_file:\n",
    "            data[folder].append(opened_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390232"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary={}#Dictionary containing words and their frequencies\n",
    "for i in range(len(data)): # For each key in newsgroup\n",
    "    for doc in data[all_folders[i]]: # For each document corresponding to key in newsgroup)\n",
    "        for word in doc.split(): # For each word in that document\n",
    "            if word.lower() not in ignore and len(word.lower()) >= 5:\n",
    "                if word.lower() not in vocabulary:\n",
    "                    vocabulary[word.lower()]=1\n",
    "                else:\n",
    "                    vocabulary[word.lower()]+=1\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocabulary=sorted(vocabulary.items(),key=operator.itemgetter(1),reverse=True)#Sorting the Vocabulary dictionary on the basis of frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list=[]#Choosing words from Vocabulary as features\n",
    "for key in sorted_vocabulary:\n",
    "    feature_list.append(key[0])\n",
    "feature_list=feature_list[0:1000]#Choosing only the first 1000 words (K=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_groups=[] #List of newsgroups to be used for splitting the training and testing data \n",
    "for i in range(len(data)):\n",
    "    for doc in data[all_folders[i]]:\n",
    "        news_groups.append(all_folders[i])\n",
    "news_groups=np.array(news_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = feature_list)\n",
    "for folder in all_folders:\n",
    "        for file in os.listdir(os.path.join(directory,folder)):# Add a new row for every file\n",
    "            df.loc[len(df)] = np.zeros(len(feature_list))\n",
    "        with open(os.path.join(directory,folder,file),encoding='latin-1') as opened_file:\n",
    "            for word in opened_file.read().split():\n",
    "                if word.lower() in feature_list:\n",
    "                    df[word.lower()][len(df)-1] += 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value=df.values#Values in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(value,news_groups,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #Sklearn module to implement Naive Bayes for multiple features\n",
    "clf=MultinomialNB()\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.score(x_test,y_test))#Score on how the well the the Naive Bayes algorithmn has worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes from scratch\n",
    "def fit(x_train,y_train):\n",
    "    result={}#Empty dictionary to store values\n",
    "    result[\"total_data\"]=len(y_train)#Key total_data storing length of y_train\n",
    "    class_=set(y_train)#Set of unique values in y_train\n",
    "    for label in class_:\n",
    "        result[label]={}\n",
    "        row=(y_train==label)\n",
    "        x_train_current=x_train[row]\n",
    "        y_train_current=y_train[row]\n",
    "        total_words=0\n",
    "        for i in range(len(feature_list)):\n",
    "            result[label][feature_list[i]]=x_train_current[:,i].sum()\n",
    "            total_words+=x_train_current[:,i].sum()\n",
    "        result[label][\"total_count\"]=total_words\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(x,dictionary,this_key):\n",
    "    output=np.log(dictionary[this_key][\"total_count\"])-np.log(dictionary[\"total_data\"])\n",
    "    for i in range(len(feature_list)):\n",
    "        num=dictionary[this_key][feature_list[i]]+1\n",
    "        dem=dictionary[this_key][\"total_count\"]+len(feature_list)\n",
    "        current_word_probability=np.log(num)-np.log(dem)#Implemented Laplace Correction as well\n",
    "        for j in range(int(x[i])):\n",
    "            output+=current_word_probability# If the frequency of word in test data point is zero then we wont consider it\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSingleClass(x,dictionary):\n",
    "    best_class=-1000\n",
    "    best_prob=-1000\n",
    "    firstRun=True\n",
    "    possible=dictionary.keys()\n",
    "    for this_key in possible:\n",
    "        if this_key==\"total_data\":\n",
    "            continue\n",
    "        this_key_probability=probability(x,dictionary,this_key)\n",
    "        if(firstRun==True or this_key_probability>best_prob):\n",
    "            best_class=this_key\n",
    "            best_prob=this_key_probability\n",
    "        firstRun=False\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test,dictionary):\n",
    "    Y_pred=[]\n",
    "    num = 0\n",
    "    for x in X_test:\n",
    "        Y_pred.append(predictSingleClass(x,dictionary))\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=predict(x_test,dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_pred,y_test))\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
